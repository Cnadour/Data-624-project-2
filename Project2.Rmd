---
title: "Project2"
date: "2024-12-09"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
  pdf_document: default
---

# Packages and Setup

```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(caret)
library(ggcorrplot)
library(gridExtra)
```

# Data Cleaning


```{r}
# Load the specific sheet from the first Excel file
training_data <- read_excel("StudentData.xlsx", sheet = "Subset")

# Load the specific sheet from the second Excel file
testing_data <- read_excel("StudentEvaluation.xlsx", sheet = "Subset (2)")

```

```{r}
# right away, remove any observations with no PH value.

training_data <-training_data[!is.na(training_data[['PH']]), ]

X <- select(training_data, -PH)
y <- training_data$PH

# no PH values are provided in studentEvaluation.xlsx, so we cannot use it
# for testing. We can only use it to make predictions. 
eval_X <- select(testing_data, -PH)
```

```{r}
# Inspect the structure of the data
glimpse(X)
glimpse(eval_X)
```

```{r}
# Check for missing values
sum(is.na(X))
sum(is.na(eval_X))
```

```{r}
# View a summary of the data
summary(X)
summary(eval_X)
```

```{r}
head(X)
head(eval_X)
```

```{r}
# Remove rows with missing data
y <- y[complete.cases(X)] 
X <- X %>% drop_na()

eval_X <- eval_X %>% drop_na()
```

```{r}
# Standardize column names
X <- X %>% rename_all(tolower) %>% 
  rename_all(gsub, pattern = " ", replacement = "_")
eval_X <- eval_X %>% rename_all(tolower) %>% 
  rename_all(gsub, pattern = " ", replacement = "_")
```

```{r}
# remove duplicates
X <- X %>% distinct()
eval_X <- eval_X %>% distinct()
```

```{r}
# remove the columns that only have 1 unique value

single_value_cols <- sapply(X, function(col) length(unique(col)) == 1)
X <- X[, !single_value_cols, drop = FALSE]
eval_X <- eval_X[, !single_value_cols, drop = FALSE]

```


```{r}
missing_train <- sapply(X, function(col) sum(is.na(col)))
missing_test <- sapply(eval_X, function(col) sum(is.na(col)))

print(missing_train)
print(missing_test)
```

```{r}
# Convert character columns to factors and ensure date columns are correctly formatted
X <- X %>%
  mutate(across(where(is.character), as.factor))

eval_X <- eval_X %>%
  mutate(across(where(is.character), as.factor))
```

```{r}
# one-hot-encode the brand_code field with dummy variables
encode_var <- function(df, col){
  ohm <- model.matrix(~ . - 1, data = df[, col, drop = FALSE])
  ohm <- as.data.frame(ohm)
  ohm <- lapply(ohm, as.factor)
  return(cbind(df[ , !names(df) %in% col, drop = FALSE], ohm))
}

X <- encode_var(X, 'brand_code')
eval_X <- encode_var(eval_X, 'brand_code')
```

```{r}
# Data types
str(X)
```




# EDA

The cell below creates a function that can be used to count the number of outliers in each column of a dataframe: 

```{r}
count_outliers <- function(dataframe) {
  outlier_counts <- sapply(dataframe, function(column) {
    if (is.numeric(column)) {
      Q1 <- quantile(column, 0.25, na.rm = TRUE)
      Q3 <- quantile(column, 0.75, na.rm = TRUE)
      IQR <- Q3 - Q1
      lower_bound <- Q1 - 1.5 * IQR
      upper_bound <- Q3 + 1.5 * IQR
      sum(column < lower_bound | column > upper_bound, na.rm = TRUE)
    } else {
      NA
    }
  })
  return(outlier_counts)
}
```

The `count_outliers` function is used below to plot the number of outliers present in each predictor field:

```{r}
outlier_counts <- count_outliers(X)
outlier_counts <- data.frame(
  Column = names(outlier_counts),
  Outliers = as.numeric(outlier_counts)
)
outlier_counts <- na.omit(outlier_counts)


ggplot(outlier_counts, aes(x = reorder(Column, -Outliers), y = Outliers)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(
    title = "# of Outliers Present in Predictor Fields",
    x = "Variable Name",
    y = "Number of Outliers"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

```

Next, the cell below includes function that tests whether or not each column within the dataframe is normal. 

```{r}
test_normality <- function(dataframe) {
  results <- lapply(dataframe, function(column) {
    if (is.numeric(column)) {
      test_result <- tryCatch(
        shapiro.test(column),
        error = function(e) NULL # handle errors (e.g., small sample size)
      )
      if (!is.null(test_result)) {
        return(data.frame(
          Statistic = test_result$statistic,
          P_Value = test_result$p.value
        ))
      } else {
        return(data.frame(Statistic = NA, P_Value = NA))
      }
    } else {
      # return NA for non-numeric columns
      return(data.frame(Statistic = NA, P_Value = NA))
    }
  })
  
  # combine results into a dataframe
  results_df <- do.call(rbind, results)
  rownames(results_df) <- names(dataframe)
  return(results_df)
}

normality_results <- test_normality(X)
normality_results
```

The cell below produces a correlation matrix to show the correlations between all pairs of predictor variables, helping to assess the level of multicollinearity. 

```{r}
corr_matrix <- cor(select(X, where(is.numeric)))
ggcorrplot(corr_matrix, lab = FALSE, title = "Correlation Matrix") +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)  
  )
```

The cell below produces scatterplots of each explanatory field with the predictor variable PH. Each scatterplot includes the correlation coefficent and a best fit line relating the two fields. 

```{r, message=FALSE}
plot_scatter_with_fit <- function(X, y) {
  plots <- list() 
  
  for (col in names(X)) {
    correlation <- cor(X[[col]], y)  
    plot <- ggplot(data = data.frame(x = X[[col]], y = y), aes(x = x, y = y)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE, color = "red") +  
      labs(
        subtitle = paste("Correlation:", round(correlation, 2)),
        x = col,
        y = "PH"
      ) +
      theme_minimal()
    plots[[col]] <- plot
  }
  
  return(plots)
}

scatter_plots <- plot_scatter_with_fit(select(X, where(is.numeric)), y)

for (plot in scatter_plots) {
  print(plot)
}
```

The cell below plots the distributions of the categorical features in `X`:

```{r, message=FALSE}
plot_categorical_distributions <- function(df) {
  # Identify categorical columns
  categorical_columns <- 
    names(df)[sapply(df, is.factor) | sapply(df, is.character)]
  
  # Create bar plots for each categorical column
  plots <- lapply(categorical_columns, function(column) {
    ggplot(df, aes_string(x = column)) +
      geom_bar(fill = "steelblue", color = "black") +
      labs(
        title = paste("Distribution of", column),
        x = column,
        y = "Count"
      ) +
      theme_minimal()
  })
  
  return(plots)
}

cat_plots <- plot_categorical_distributions(select(X, where(is.factor)))

for (plot in cat_plots) {
  print(plot)
}
```

# Data Pre-Processing

Split the data into testing and training sets. 

```{r}
train_index <- createDataPartition(y, p = 0.75, list = FALSE)

X_train <- X[train_index, ]
X_test <- X[-train_index, ]

y_train <- y[train_index]
y_test <- y[-train_index]
```

Apply the Yeo-Johnson transformation. 

```{r}
numeric_features <- sapply(X, is.numeric)

# fit the Yeo-Johnson transformation using training data
preprocess_params <- preProcess(X_train[, numeric_features],
                                method = "YeoJohnson")

# pply the transformation to training and testing data
X_train[, numeric_features] <- predict(preprocess_params,
                                       X_train[, numeric_features])
X_test[, numeric_features] <- predict(preprocess_params,
                                    X_test[, numeric_features])
```

Use robust scaling to scale the data:

```{r}
# only use medians and iqrs from training data
medians <- apply(X_train[, numeric_features], 2, median)
iqrs <- apply(X_train[, numeric_features], 2, IQR)

# performs the robust scaling using the IQRs and medians
robust_scale <- function(data, medians, iqrs) {
  scaled_data <- sweep(data, 2, medians, "-")  
  scaled_data <- sweep(scaled_data, 2, iqrs, "/") 
  return(scaled_data)
}

# transform the training and testing data
X_train[, numeric_features] <- robust_scale(X_train[, numeric_features],
                                           medians, iqrs)
X_test[, numeric_features] <- robust_scale(X_test[, numeric_features], 
                                          medians, iqrs)
```

Finds all pairs of highly correlated variables ($|r| > 0.7$) and randomly removes one variable from each pair. 

```{r}
high_corr_indices <- findCorrelation(corr_matrix, cutoff = 0.7, names = TRUE)
X_train <- X_train[, !names(X_train) %in% high_corr_indices]
X_test <- X_test[, !names(X_test) %in% high_corr_indices]
```

Make new variables (and update old ones) using the transformed data:

```{r}
X <- rbind(X_train, X_test)
train <- X_train
train$PH <- y_train
test <- X_test
test$PH <- y_test
```

Check again for outliers:

```{r}
outlier_counts <- count_outliers(X)
outlier_counts <- data.frame(
  Column = names(outlier_counts),
  Outliers = as.numeric(outlier_counts)
)
outlier_counts <- na.omit(outlier_counts)


ggplot(outlier_counts, aes(x = reorder(Column, -Outliers), y = Outliers)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(
    title = "# of Outliers Present in Predictor Fields",
    x = "Variable Name",
    y = "Number of Outliers"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Check again for normal distributions:

```{r}
normality_results <- test_normality(X)
normality_results
```

Check again for multicollinearity:

```{r}
corr_matrix <- cor(select(X, where(is.numeric)))
ggcorrplot(corr_matrix, lab = FALSE, title = "Correlation Matrix") +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)  
  )
```



