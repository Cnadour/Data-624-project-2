---
title: "Project2"
date: "2024-12-09"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
  pdf_document: default
---

# Packages and Setup

```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(caret)
library(ggcorrplot)
library(gridExtra)
```

# Data Cleaning


```{r}
# Load the specific sheet from the first Excel file
training_data <- read_excel("StudentData.xlsx", sheet = "Subset")

# Load the specific sheet from the second Excel file
testing_data <- read_excel("StudentEvaluation.xlsx", sheet = "Subset (2)")

```

```{r}
# right away, remove any observations with no PH value.

training_data <-training_data[!is.na(training_data[['PH']]), ]

X <- select(training_data, -PH)
y <- training_data$PH

# no PH values are provided in studentEvaluation.xlsx, so we cannot use it
# for testing. We can only use it to make predictions. 
eval_X <- select(testing_data, -PH)
```

```{r}
# Inspect the structure of the data
glimpse(X)
glimpse(eval_X)
```

```{r}
# Check for missing values
sum(is.na(X))
sum(is.na(eval_X))
```

```{r}
# View a summary of the data
summary(X)
summary(eval_X)
```

```{r}
head(X)
head(eval_X)
```

```{r}
# Remove rows with missing data
y <- y[complete.cases(X)] 
X <- X %>% drop_na()

eval_X <- eval_X %>% drop_na()
```

```{r}
# Standardize column names
X <- X %>% rename_all(tolower) %>% 
  rename_all(gsub, pattern = " ", replacement = "_")
eval_X <- eval_X %>% rename_all(tolower) %>% 
  rename_all(gsub, pattern = " ", replacement = "_")
```

```{r}
# remove duplicates
X <- X %>% distinct()
eval_X <- eval_X %>% distinct()
```

```{r}
# remove the columns that only have 1 unique value

single_value_cols <- sapply(X, function(col) length(unique(col)) == 1)
X <- X[, !single_value_cols, drop = FALSE]
eval_X <- eval_X[, !single_value_cols, drop = FALSE]

```


```{r}
missing_train <- sapply(X, function(col) sum(is.na(col)))
missing_test <- sapply(eval_X, function(col) sum(is.na(col)))

print(missing_train)
print(missing_test)
```

```{r}
# Convert character columns to factors and ensure date columns are correctly formatted
X <- X %>%
  mutate(across(where(is.character), as.factor))

eval_X <- eval_X %>%
  mutate(across(where(is.character), as.factor))
```

```{r}
# one-hot-encode the brand_code field with dummy variables
encode_var <- function(df, col){
  ohm <- model.matrix(~ . - 1, data = df[, col, drop = FALSE])
  ohm <- as.data.frame(ohm)
  ohm <- lapply(ohm, as.factor)
  return(cbind(df[ , !names(df) %in% col, drop = FALSE], ohm))
}

X <- encode_var(X, 'brand_code')
eval_X <- encode_var(eval_X, 'brand_code')
```

```{r}
# Data types
str(X)
```




# EDA

The cell below creates a function that can be used to count the number of outliers in each column of a dataframe: 

```{r}
count_outliers <- function(dataframe) {
  outlier_counts <- sapply(dataframe, function(column) {
    if (is.numeric(column)) {
      Q1 <- quantile(column, 0.25, na.rm = TRUE)
      Q3 <- quantile(column, 0.75, na.rm = TRUE)
      IQR <- Q3 - Q1
      lower_bound <- Q1 - 1.5 * IQR
      upper_bound <- Q3 + 1.5 * IQR
      sum(column < lower_bound | column > upper_bound, na.rm = TRUE)
    } else {
      NA
    }
  })
  return(outlier_counts)
}
```

The `count_outliers` function is used below to plot the number of outliers present in each predictor field:

```{r}
outlier_counts <- count_outliers(X)
outlier_counts <- data.frame(
  Column = names(outlier_counts),
  Outliers = as.numeric(outlier_counts)
)
outlier_counts <- na.omit(outlier_counts)


ggplot(outlier_counts, aes(x = reorder(Column, -Outliers), y = Outliers)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(
    title = "# of Outliers Present in Predictor Fields",
    x = "Variable Name",
    y = "Number of Outliers"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

```

Next, the cell below includes function that tests whether or not each column within the dataframe is normal. 

```{r}
test_normality <- function(dataframe) {
  results <- lapply(dataframe, function(column) {
    if (is.numeric(column)) {
      test_result <- tryCatch(
        shapiro.test(column),
        error = function(e) NULL # handle errors (e.g., small sample size)
      )
      if (!is.null(test_result)) {
        return(data.frame(
          Statistic = test_result$statistic,
          P_Value = test_result$p.value
        ))
      } else {
        return(data.frame(Statistic = NA, P_Value = NA))
      }
    } else {
      # return NA for non-numeric columns
      return(data.frame(Statistic = NA, P_Value = NA))
    }
  })
  
  # combine results into a dataframe
  results_df <- do.call(rbind, results)
  rownames(results_df) <- names(dataframe)
  return(results_df)
}

normality_results <- test_normality(X)
normality_results
```

The cell below produces a correlation matrix to show the correlations between all pairs of predictor variables, helping to assess the level of multicollinearity. 

```{r}
corr_matrix <- cor(select(X, where(is.numeric)))
ggcorrplot(corr_matrix, lab = FALSE, title = "Correlation Matrix") +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)  
  )
```

The cell below produces scatterplots of each explanatory field with the predictor variable PH. Each scatterplot includes the correlation coefficent and a best fit line relating the two fields. 

```{r, message=FALSE}
plot_scatter_with_fit <- function(X, y) {
  plots <- list() 
  
  for (col in names(X)) {
    correlation <- cor(X[[col]], y)  
    plot <- ggplot(data = data.frame(x = X[[col]], y = y), aes(x = x, y = y)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE, color = "red") +  
      labs(
        subtitle = paste("Correlation:", round(correlation, 2)),
        x = col,
        y = "PH"
      ) +
      theme_minimal()
    plots[[col]] <- plot
  }
  
  return(plots)
}

scatter_plots <- plot_scatter_with_fit(select(X, where(is.numeric)), y)

for (plot in scatter_plots) {
  print(plot)
}
```

The cell below plots the distributions of the categorical features in `X`:

```{r, message=FALSE}
plot_categorical_distributions <- function(df) {
  # Identify categorical columns
  categorical_columns <- 
    names(df)[sapply(df, is.factor) | sapply(df, is.character)]
  
  # Create bar plots for each categorical column
  plots <- lapply(categorical_columns, function(column) {
    ggplot(df, aes_string(x = column)) +
      geom_bar(fill = "steelblue", color = "black") +
      labs(
        title = paste("Distribution of", column),
        x = column,
        y = "Count"
      ) +
      theme_minimal()
  })
  
  return(plots)
}

cat_plots <- plot_categorical_distributions(select(X, where(is.factor)))

for (plot in cat_plots) {
  print(plot)
}
```

# Data Pre-Processing


Step 1: Identify numeric columns for transformations

```{r}
# Step 1: Identify numeric columns for transformations
numeric_columns <- X %>% select_if(is.numeric)
```

Step 2: Apply Yeo-Johnson transformation (for non-normality)

```{r}
preProcess_numeric <- preProcess(numeric_columns, method = "YeoJohnson")
X[names(numeric_columns)] <- predict(preProcess_numeric, numeric_columns)
```

Step 3: Check for outliers using interquartile range (IQR) method

```{r}
check_outliers <- function(df) {
  df %>%
    summarise(across(where(is.numeric), ~ sum(. > (quantile(., 0.75) + 1.5 * IQR(.)) |
                                      . < (quantile(., 0.25) - 1.5 * IQR(.)), na.rm = TRUE)))
}

outlier_counts <- check_outliers(X)
outlier_counts
```

Step 4: Scale numeric data using robust scaling (median and IQR)

```{r}
robust_scale <- function(df) {
  df %>% mutate(across(where(is.numeric), ~ (. - median(.)) / IQR(.), .names = "{.col}_scaled"))
}


X <- robust_scale(X)
```

Step 5: Check for multicollinearity using correlation matrix

```{r}
cor_matrix <- X %>% select_if(is.numeric) %>% cor()
```

Visualize the correlation matrix

```{r}
cor_matrix_plot <- cor_matrix %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  pivot_longer(-Variable, names_to = "Correlated_Variable", values_to = "Correlation") %>%
  filter(Variable != Correlated_Variable, abs(Correlation) > 0.7) %>%
  ggplot(aes(x = Variable, y = Correlated_Variable, fill = Correlation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Matrix", x = "", y = "")

print(cor_matrix_plot)
```

Identify highly correlated variables (threshold = 0.7)

```{r}
find_highly_correlated <- function(cor_matrix, threshold = 0.7) {
  cor_pairs <- as.data.frame(as.table(cor_matrix)) %>%
    filter(Var1 != Var2, abs(Freq) > threshold)
  cor_pairs
}
high_corr <- find_highly_correlated(cor_matrix)
high_corr
```


Drop one variable from each highly correlated pair:


```{r}
vars_to_remove <- unique(high_corr$Var2) # Replace with actual variable names if needed
# X <- X %>% select(-all_of(vars_to_remove, .env = environment()))
```


Step 6: Reassess normality, outlier counts, and multicollinearity. Normality after transformation: 

```{r}
shapiro_test_results <- lapply(X %>% select_if(is.numeric), shapiro.test)
normality_summary <- sapply(shapiro_test_results, function(x) x$p.value)
normality_summary
```

Outlier reassessment: 

```{r}
outlier_counts_after <- check_outliers(X)
outlier_counts_after
```

Correlation reassessment:

```{r}
cor_matrix_after <- X %>% select_if(is.numeric) %>% cor()
```

Ensure multicollinearity is addressed: 

```{r}
cor_matrix_plot_after <- cor_matrix_after %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  pivot_longer(-Variable, names_to = "Correlated_Variable", values_to = "Correlation") %>%
  filter(Variable != Correlated_Variable, abs(Correlation) > 0.7) %>%
  ggplot(aes(x = Variable, y = Correlated_Variable, fill = Correlation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Matrix (After Adjustment)", x = "", y = "")

print(cor_matrix_plot_after)
```

Save the preprocessed data for modeling: 
```{r}
# Save the preprocessed data for modeling
write.csv(X, "preprocessed_student_data.csv", row.names = FALSE)
```



